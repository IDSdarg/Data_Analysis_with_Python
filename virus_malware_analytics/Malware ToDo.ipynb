{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Virus, Malware and Executables : Features Functions to implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descriptive Stats | Inferential Stats| Predictive Stats\n",
    "--|--|--\n",
    "Description or summary of features of data we have at hand. | Make generalization based on sample data we have at hand. | Make future projections based on data we have at hand.\n",
    "\n",
    "\n",
    "* Cluster all `IP` by `classes` (A,B,C) and check `Descriptive Stats` of each class using: <br> `(Boxplot, Q-Q plot, Histogram, ANOVA)`. Bloxplot to show median, quantile etc. *Var: IPs, IP Classes, Continents, Countries, Cities, malware, domains, etc* \n",
    "\n",
    "\n",
    "* Can we approximate the IPs distribution as a `normal distribution`? How about each `IP Class`? Are they approximately normal? Use `Q-Q plot` to show relationship for each class (law of large numbers and central limit theorem).\n",
    "\n",
    "\n",
    "* Does the `IP Class` that an IP belongs to affect its vulnerability to attack? Use `F-Ratio test ANOVA`. If true, which class is potentially more vulnerable?\n",
    "\n",
    "\n",
    "* Measure time effect (by days or weeks) `dayselapsed` on the `frequency` of attack on the DNS. `Cluster` by `IPclass`. Use scatterplot for days, boxplot for weeks.\n",
    "\n",
    "\n",
    "* Which `domain | IP` has `highest & Lowest` vulnerability or most targetted DNS? `Boxplot` and `Bubbleplot`\n",
    "\n",
    "\n",
    "* Which virus/malware `(MD5Hash)` has the `highest & Lowest` penetration frequency? `Boxplot` and `Bubbleplot`\n",
    "\n",
    "\n",
    "* Use `IP LookUp API` to determine geographic location of the IP. Map geopgraphic location to world map. `Country, long/lat, Continent, etc`\n",
    "\n",
    "\n",
    "* Plot charts by `Country, Continent, Class, City, State, Type, ISP` etc using `Boxplot` and `Bubbleplot`\n",
    "\n",
    "\n",
    "* Scrape offline and Online blacklisted IP and create Histograms of IPs in our dataset identified as blacklisted.\n",
    "\n",
    "\n",
    "* Query `IP` of incoming users (new data), use as input to `IP LookUP API` and obtain feature set data about the user. Based on the `Class`, `predict` which virus/malware `MD5Hash` the user IP is vulnerable to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `IP LookUp API` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an `IP Address`,the `IP LookUp` API extracts `CountryName`, `CountryCode`, `City`, `State`, `CityLocation (Long/Lat)`, `CountryLocation (Long/Lat)`, `Zipcode` and `IP StatusMessage` as `JSON()` format. The project can also aid in `Fraud detection`\n",
    "\n",
    "* https://github.com/fiorix/freegeoip\n",
    "* http://freegeoip.net/\n",
    "\n",
    "Request: http://freegeoip.net/json/70.173.245.105\n",
    "\n",
    "Return Values\n",
    "`{\n",
    "\"ip\": \"70.173.245.105\",\n",
    "\"country_code\": \"US\",\n",
    "\"country_name\": \"United States\",\n",
    "\"region_code\": \"NV\",\n",
    "\"region_name\": \"Nevada\",\n",
    "\"city\": \"Las Vegas\",\n",
    "\"zip_code\": \"89108\",\n",
    "\"time_zone\": \"America/Los_Angeles\",\n",
    "\"latitude\": 36.2143,\n",
    "\"longitude\": -115.2131,\n",
    "\"metro_code\": 839\n",
    "}`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IP Geolocation Data Preparation & Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, time, threading, requests\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Locate API key for the geoips application - www.geoips.com\n",
    "f_entr = open(os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), 'geoips_enterp_key.txt'), 'r')\n",
    "f_basc = open(os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), 'geoips_basic_key.txt'), 'r')\n",
    "\n",
    "geoips_key = f_entr.read()\n",
    "# geoips_key = f_basc.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare `input` data file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break the `400,000` rows of unique IPs into `csv` files of size `9,000` rows\n",
    "\n",
    "`FREEGEOIP` allows up to 10,000 queries per hour by default. Once this limit is reached, all requests will result in HTTP 403, forbidden, until 1 hour quota is cleared. To avoid `403` and other server error, we could setup a `cronjob` to automatically query the `API` service every hour at a lower rate of `9,000 queries/hr.` First, we need to divide the `000,000` rows of unique IPs to `0,000`. For tractability, we set the file name as the `file_day_hr` e.g. `free_geo_19_01`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Function to determine the `IP Class`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assign_ipclass(ip):\n",
    "    \"\"\" Assign class A,B,C,D,E or Special to an IP address. \"\"\"\n",
    "    \n",
    "    # Split IP and take first and second octects.\n",
    "    first_octet = int(ip.split(sep='.')[0])\n",
    "    scnd_octet = int(ip.split(sep='.')[1])\n",
    "    \n",
    "    # Determine class of IP based on 1st and 2nd octet\n",
    "    if (0 <= first_octet <= 126):\n",
    "        if (first_octet == 10):\n",
    "            return 'Private Class A'\n",
    "        else:\n",
    "            return 'Class A'\n",
    "    \n",
    "    elif (first_octet == 127):\n",
    "        return 'Loopback IP' \n",
    "    \n",
    "    elif (128 <= first_octet <= 191):\n",
    "        if scnd_octet in np.arange(16,31):\n",
    "            return 'Private Class B'\n",
    "        else:\n",
    "            return 'Class B'\n",
    "        \n",
    "    elif (192 <= first_octet <= 223):\n",
    "        if (scnd_octet == 168):\n",
    "            return 'Private Class C'\n",
    "        else:\n",
    "            return 'Class C'\n",
    "    \n",
    "    elif (224 <= first_octet <= 239):\n",
    "        return 'Class D'\n",
    "    \n",
    "    elif (240 <= first_octet <= 255):\n",
    "        return 'Class E'\n",
    "    else:\n",
    "        return 'Unknown IP'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update `IP Address`  with the `Classes` and display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Assign classes to IPs\n",
    "uniqueIPs['IP_Class'] = uniqueIPs.IP_Address.apply(lambda ip: assign_ipclass(ip))\n",
    "\n",
    "# Check the Classes assigned\n",
    "print('\\nAssigned IPs : ',uniqueIPs.IP_Class.unique())\n",
    "uniqueIPs.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "API calls required : 5 \n"
     ]
    }
   ],
   "source": [
    "api_hr_limit = 9500\n",
    "uniqueIPs = pd.read_csv(\"uniqueIPs.csv\", index_col=0)\n",
    "\n",
    "# Uniques processed with `geoips API` - 200K\n",
    "uniques_done = 354000\n",
    "# uniqueIPs[37500:uniques_done].to_csv(\"for_geoips.csv\")\n",
    "\n",
    "# This uniques will be processed by `free_geoip` approx. 200K\n",
    "uniqueIPs = uniqueIPs[uniques_done:]\n",
    "\n",
    "# Expected number of API calls for the List of IPs we want to extract geodata for.\n",
    "total_api_call = round(uniqueIPs.index.size / api_hr_limit)\n",
    "print(\"\\nAPI calls required : %d \" %total_api_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>days</th>\n",
       "      <th>hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   days  hours\n",
       "0     5     23\n",
       "1     6      2\n",
       "2     6      5\n",
       "3     6      8\n",
       "4     6     11"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify date and time that cron job will begin\n",
    "\n",
    "total_api_call = 7\n",
    "def make_dates():\n",
    "    \"\"\" Comments\"\"\"\n",
    "    date_rng = pd.Series(pd.date_range(start=pd.datetime.today(), periods=total_api_call, freq='3H'))\n",
    "    rng_dt = pd.DataFrame()\n",
    "    rng_dt['days'] = date_rng.apply(lambda x: x.day)\n",
    "    rng_dt['hours'] = date_rng.apply(lambda x: x.hour)\n",
    "    return rng_dt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Call the function\n",
    "rg_dates = make_dates()\n",
    "    \n",
    "rg_dates.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the rest on `freegeoip.net`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set filenames\n",
    "inp_filenames = [\"free_gip_{0}_{1}.csv\".format(tm[0],tm[1]) for tm in rg_dates.values]\n",
    "# print(inp_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Divide the dataframes of IPs into separate .csv files. \n",
    "\n",
    "p_start = 0\n",
    "p_end = api_hr_limit \n",
    "\n",
    "inp_ip_file_path = os.path.join(os.getcwd(), 'cron_data', 'input')\n",
    "\n",
    "\n",
    "for f_name in inp_filenames:\n",
    "    data_extract = uniqueIPs[p_start:p_end]\n",
    "    data_extract.to_csv(os.path.join(inp_ip_file_path, f_name))\n",
    "    p_start = p_end\n",
    "    p_end += api_hr_limit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to `API`, fetch data using `freegeoip API`. Save as `Output` data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>country_code</th>\n",
       "      <th>country_name</th>\n",
       "      <th>ip</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>metro_code</th>\n",
       "      <th>region_code</th>\n",
       "      <th>region_name</th>\n",
       "      <th>time_zone</th>\n",
       "      <th>zip_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hangzhou</td>\n",
       "      <td>CN</td>\n",
       "      <td>China</td>\n",
       "      <td>115.28.44.153</td>\n",
       "      <td>30.2936</td>\n",
       "      <td>120.1614</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>Zhejiang Sheng</td>\n",
       "      <td>Asia/Shanghai</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>GB</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>109.74.205.147</td>\n",
       "      <td>51.5000</td>\n",
       "      <td>-0.1300</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Europe/London</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       city country_code    country_name              ip  latitude  longitude  \\\n",
       "0  Hangzhou           CN           China   115.28.44.153   30.2936   120.1614   \n",
       "1                     GB  United Kingdom  109.74.205.147   51.5000    -0.1300   \n",
       "\n",
       "   metro_code region_code     region_name      time_zone zip_code  \n",
       "0           0          33  Zhejiang Sheng  Asia/Shanghai           \n",
       "1           0                              Europe/London           "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lookup_ip_geodata(list_of_ips):\n",
    "    \"\"\" Function to lookup geolocation data of given IP addresses. \n",
    "        Returns dataframe of geolocation for given IPs.\n",
    "    \"\"\"\n",
    "    ip_lookup = []\n",
    "    for ip in list_of_ips:\n",
    "        r = requests.get(\"http://freegeoip.net/json/\"+ip)\n",
    "        if r.ok:\n",
    "            ip_lookup.append(r.json())\n",
    "    return pd.DataFrame(ip_lookup)\n",
    "\n",
    "\n",
    "\n",
    "# Test the function\n",
    "lookup_ip_geodata(uniqueIPs.IP_Address.values[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['free_gip_5_20.csv']\n"
     ]
    }
   ],
   "source": [
    "inp_filenames = ['free_gip_6_0.csv']\n",
    "print(inp_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Code to execute at every hour`. \n",
    "With the `csv` datafiles all ready. We need to configure a `cronjob` that executes at regular `1 hour` interval. At each hour, the cron reads a `csv` file and perform a `GET request` for all `input IPs` in the respective `csv` file. On completion, it saves the `IP geodata` fetched through `API` into another `csv` file as output. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files_treated = []\n",
    "out_geo_ip_file_path = os.path.join(os.getcwd(), 'cron_data', 'output')\n",
    "\n",
    "\n",
    "def readip_lookupgeo_out2file():\n",
    "    \"\"\" This fxn reads each .csv file as input. It calls the `lookup_ip_geodata`\n",
    "        which returns a dataframe of geo data for set of IPs in the csv file. \n",
    "        The dataframe is then resaved as .csv output file. \n",
    "    \"\"\"\n",
    "    start_day = pd.datetime.today().day\n",
    "    start_hr = pd.datetime.today().hour\n",
    "#     start_min = round(pd.datetime.today().minute, -1)\n",
    "    file_to_pull = \"free_gip_{0}_{1}.csv\".format(start_day,start_hr)\n",
    "    \n",
    "    # Check if file exist and read it. Else print msg to log and exit function\n",
    "    file_url = os.path.join(inp_ip_file_path, file_to_pull)\n",
    "    \n",
    "    if not os.path.isfile(file_url):\n",
    "        print(\"File '{}' does not exist!\".format(file_to_pull))\n",
    "        return \n",
    "    elif file_to_pull in files_treated:\n",
    "        print(\"File '{}' already fetched.\".format(file_to_pull))\n",
    "        return \n",
    "    else:\n",
    "        try:\n",
    "            inp_ips_addrs = pd.read_csv(file_url, index_col=0).IP_Address.values\n",
    "            ip_geo_df = lookup_ip_geodata(inp_ips_addrs)\n",
    "            files_treated.append(file_to_pull)\n",
    "        except ConnectionError or MaxRetryError:\n",
    "            print(\"Connection Problem on '{}'. Moving on.\".format(file_to_pull))\n",
    "            return\n",
    "    \n",
    "    file_fetched = \"out_free_gip_{0}_{1}.csv\".format(start_day, start_hr)\n",
    "    out_path = os.path.join(out_geo_ip_file_path, file_fetched)\n",
    "    ip_geo_df.to_csv(out_path)\n",
    "    \n",
    "    print(\"{0}: {1} of {2} successfully completed!\".format(\n",
    "            file_fetched, inp_filenames.index(file_to_pull)+1, len(inp_filenames)))\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup `cronjob` or `timer` function to call `readip_lookupgeo_out2file()` every `65 mins`\n",
    "\n",
    "Function below calls `readip_lookupgeo_out2file()` every `65 mins` converted into secs. We set a timer that does the counting and continue the loop till all the `csv` input files have been treated. This implies that we loop till end of `len(filenames)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def lookup_ip_geo_timer():\n",
    "    \"\"\" Continuosly call function till all filenames are treated. \"\"\"\n",
    "    for fn in inp_filenames:\n",
    "        readip_lookupgeo_out2file()\n",
    "        time.sleep(1800)                 # 60 mins interval\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the timer function that pulls data using `API`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Call function and print progress\n",
    "\n",
    "lookup_ip_geo_timer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to `Update` missing values in data extracted from `FreeGeoIP`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "gips_base_url = \"http://api.geoips.com/ip\"\n",
    "append_url = \"output/json/hostname/true/timezone/true\"\n",
    "\n",
    "\n",
    "def update_free_geoip_data(df):\n",
    "    geoips_url = \"{0}/{1}/key/{2}/{3}\".format(gips_base_url, df.name, geoips_key, append_url)\n",
    "    r = requests.get(geoips_url)\n",
    "    \n",
    "    if r.ok and 'error' not in r.json():\n",
    "        returned_data = r.json()['response']\n",
    "        \n",
    "        # Get request ok, no error but no entry was found\n",
    "        if 'Success' not in returned_data['message']:\n",
    "            print(returned_data['notes'])\n",
    "            return \n",
    "        # Request is ok and success msg was received.\n",
    "        else:\n",
    "            ip_location_data = returned_data['location']\n",
    "            \n",
    "            # Rename keys to match existing dataframe naming format\n",
    "            ip_location_data.pop('county_name')\n",
    "            ip_location_data.pop('ip')\n",
    "            ip_location_data['city'] = ip_location_data.pop('city_name')\n",
    "            ip_location_data['time_zone'] = ip_location_data.pop('timezone')\n",
    "            \n",
    "            # Update missing data in dataframe with fetched API data\n",
    "            for fetched_key, fetched_value in ip_location_data.items():\n",
    "                if pd.isnull(df[fetched_key]):\n",
    "                    df[fetched_key] = fetched_value  \n",
    "                    \n",
    "            df['latitude'] = ip_location_data['latitude']\n",
    "            df['longitude'] = ip_location_data['longitude']  \n",
    "\n",
    "    # request ok but something else went wrong with the fetch\n",
    "    else:\n",
    "        stop_point_log.append(\"API stopped at {0}\".format(df.name))\n",
    "        print(r.json()['error']['notes'])\n",
    "        return\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data file of  `IP Geo LookUp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Array to store log information about error during API calls and fetches\n",
    "stop_point_log = []      \n",
    "\n",
    "# Temp variables to be deleted when function moves\n",
    "out_geo_ip_file_path = os.path.join(os.getcwd(), 'cron_data', 'output')\n",
    "\n",
    "\n",
    "\n",
    "def loadcsv_call_updater(csv_fname):\n",
    "    \"\"\" The function loads a CSV file given in the filename, \n",
    "        changes it structure and call the update function on it \n",
    "    \"\"\"\n",
    "    # Load dataframe from csv\n",
    "    df = pd.read_csv(os.path.join(out_geo_ip_file_path, csv_fname), index_col='ip')\n",
    "    df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "    \n",
    "    # Update structure of existing dataframe\n",
    "    df['continent_name'] = None\n",
    "    df['continent_code'] = None\n",
    "    df['owner'] = None\n",
    "    df['hostname'] = None\n",
    "\n",
    "    # Get those rows missing some data properties\n",
    "    has_missing_data = df[df.country_name.isnull()]\n",
    "\n",
    "    # Update missing rows with new data from API calls \n",
    "    updated_row = has_missing_data.apply(update_free_geoip_data, axis=1)\n",
    "    \n",
    "    # Merge inplace into original dataframe using .update() and resave as .csv file into `phase2` directory.\n",
    "    df.update(updated_row)\n",
    "    df.to_csv(os.path.join(os.getcwd(), 'cron_data', 'phase2' ,csv_fname))\n",
    "    print(\"{} Done!\".format(csv_fname))\n",
    "    return \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function\n",
    "Function here iteratively calls `loadcsv_call_updater(csv_fname)` which in turn calls `update_free_geoip_data(df)`. Both together load a `.csv` containing output rows of geolocation data on an IP int odataframe.  The `loadcsv_call_updater(csv_fname)` examines the dataframe for rows that contain missing data properties. It extract the missing elements into a temporary dataframe and calls `df.apply(update_free_geoip_data)` on each of the rows. The `update_free_geoip_data()` uses another `IP geolocation API service` to extract finer data. (So we perform API data extraction twice!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Consider the filenames of the `/output/out_free_gip_XX_YY_ZZ.csv` and format a uniform call mthd\n",
    "\n",
    "\n",
    "out_filenames = [\"out_free_gip_{0}_{1}_{2}.csv\".format(tm[0],tm[1],tm[2]) for tm in rg_dates.values]\n",
    "\n",
    "for csv_file in out_filenames:\n",
    "    loadcsv_call_updater(csv_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge `API` data file with the data file containing `IP class` info\n",
    "\n",
    "Merge all the API data files into single dataframe and save as `.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pieces = []\n",
    "\n",
    "for f_csv in out_filenames[:2]:\n",
    "    api_file = pd.read_csv(os.path.join(os.getcwd(), 'cron_data', 'phase2', f_csv), index_col=0)\n",
    "    pieces.append(api_file)\n",
    "\n",
    "\n",
    "# Concatenate the list of files into single dataframe\n",
    "ip_geodata_lookup = pd.concat(pieces) \n",
    "\n",
    "# Reaarange the columns\n",
    "column_names = ['ip', 'country_name', 'country_code', 'latitude', 'longitude', \n",
    "                'region_name', 'region_code', 'city', 'metro_code', 'zip_code', 'time_zone']\n",
    "\n",
    "ip_geodata_lookup.columns = column_names\n",
    "\n",
    "\n",
    "# Merge with the `uniqueIPs` data file containing CLASS information\n",
    "augmented_ip_geodata = pd.merge(uniqueIPs, ip_geodata_lookup, left_on='IP_Address', right_on='ip')\n",
    "augmented_ip_geodata.drop('ip', axis=1, inplace=True)\n",
    "\n",
    "augmented_ip_geodata.to_csv(\"augmented_ip_geodata.csv\")\n",
    "augmented_ip_geodata.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
